{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1923617c-e69e-479e-a0dd-3f2079abddaf",
   "metadata": {},
   "source": [
    "# Project 16 - Building a Spam Filter with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46969565-6837-4b97-bbc5-58d39ab6850f",
   "metadata": {},
   "source": [
    "In this guided project, we're going to study the practical side of Naive Bayes algorithm by building a spam filter for SMS messages.\n",
    "\n",
    "Our first task is to \"teach\" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the The [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/228/sms+spam+collection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a2e706-cd29-47a5-bb21-3a6eaccd382a",
   "metadata": {},
   "source": [
    "## Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25931b2-59f4-4f21-92c2-4668e662e506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sms_spam = pd.read_csv('sms+spam+collection/SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])\n",
    "\n",
    "print(sms_spam.shape)\n",
    "sms_spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038dfac0-6ec4-44e9-aa6a-49135e87dfc1",
   "metadata": {},
   "source": [
    "Our dataset has 5572 rows, meaning that there are 5572 different messages. On the `Label` column we have information about the message: if the message is spam, the label is `spam`, if it is not a spam message, then the label is `ham`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd864608-d75d-40fd-8021-bf2673b465a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_spam['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b354b-814b-4195-a280-fb0aff309061",
   "metadata": {},
   "source": [
    "From the frequency table above we can see that 13.4% of the messages are spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cebf652-5500-4d7f-8291-0fcef8eb679e",
   "metadata": {},
   "source": [
    "## Training and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24160423-a116-48cd-993b-0264ba337edc",
   "metadata": {},
   "source": [
    "Once our spam filter is done, we'll need to test how good it is with classifying new messages. To test the spam filter, we're first going to split our dataset into two categories:\n",
    "\n",
    "- <b>A training set</b> is used to \"train\" the computer how to classify messages\n",
    "- <b>A test set</b> is used to test how good the spam filter is with classifying new messages\n",
    "\n",
    "We're going to keep 80% of our dataset for training, and 20% for testing.\n",
    "\n",
    "<b>The goal of this project is to create a spam filter that classifies new messages with accuracy greater than 80%. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226f267f-e25a-4573-9938-5d9f96949859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4458, 2)\n",
      "(1114, 2)\n",
      "  Label                                            SMS\n",
      "0   ham                   Yep, by the pretty sculpture\n",
      "1   ham  Yes, princess. Are you going to make me moan?\n",
      "     Label                                                SMS\n",
      "4456   ham  Hello my boytoy ... Geeee I miss you already a...\n",
      "4457   ham                           Wherre's my boytoy ? :-(\n"
     ]
    }
   ],
   "source": [
    "# Randomize the dataset\n",
    "data_randomized = sms_spam.sample(frac=1, random_state=1)\n",
    "\n",
    "# Calculate index for split\n",
    "training_test_index = round(len(data_randomized) * 0.8)\n",
    "\n",
    "# Training/Test split\n",
    "training_set = data_randomized[:training_test_index].reset_index(drop=True)\n",
    "testing_set = data_randomized[training_test_index:].reset_index(drop=True)\n",
    "\n",
    "print(training_set.shape)\n",
    "print(testing_set.shape)\n",
    "\n",
    "print(training_set.head(2))\n",
    "print(training_set.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3228f59b-5a54-4bc0-9ddb-007ed5f40b76",
   "metadata": {},
   "source": [
    "Let's next find out the percentage of spam and ham in both the training and the test set to find out if the percentages are similar to what they were earlier in the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd55b1ca-280e-404d-91b8-d4fe38b82b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     0.86541\n",
      "spam    0.13459\n",
      "Name: proportion, dtype: float64\n",
      "Label\n",
      "ham     0.868043\n",
      "spam    0.131957\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(training_set['Label'].value_counts(normalize=True))\n",
    "print(testing_set['Label'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ac5fb-e5ab-4727-9a99-9b74023e67af",
   "metadata": {},
   "source": [
    "The percentages are close to the percentages of full dataset (13.4%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd21639-d061-4135-a6c9-d0f1ad55c7c1",
   "metadata": {},
   "source": [
    "## Letter Case and Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb76de-e1ce-46cb-ba50-9d464c56388f",
   "metadata": {},
   "source": [
    "The next step is to use the training set to teach the algorithm to classify new messages. To calculate the probabilities needed for filtering, we will first need to clean the data. For example some messages look like this:\n",
    "|label|SMS|\n",
    "|-----|---|\n",
    "|spam|SECRET PRIZE! CLAIM SECRET PRICE NOW!!|\n",
    "|ham|Coming to my secret party?|\n",
    "|spam|Winner! Claim secret prize now!|\n",
    "\n",
    "We want to make the data look like this:\n",
    "\n",
    "|label|secret|price|claim|now|coming|to|my|party|winner|\n",
    "|-----|------|-----|-----|---|------|--|--|-----|------|\n",
    "|spam|2|2|1|1|0|0|0|0|0|\n",
    "|ham|1|0|0|0|1|1|1|1|0|\n",
    "|spam|1|1|1|1|0|0|0|0|1|\n",
    "\n",
    "So instead of having `SMS` column, we want a series of new columns that contain unique words (in lower case), and each row has a count of those words for that message. We will also delete all special marks, like `!!!` and `,`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a0cb21f-65b3-49b4-b00c-265eae08a053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yep, by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes, princess. Are you going to make me moan?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Havent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I forgot 2 ask ü all smth.. There's a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       Yep, by the pretty sculpture\n",
       "1   ham      Yes, princess. Are you going to make me moan?\n",
       "2   ham                         Welp apparently he retired\n",
       "3   ham                                            Havent.\n",
       "4   ham  I forgot 2 ask ü all smth.. There's a card on ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before cleaning\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6fb8f8-7f80-4705-b7e4-2703cb9ddb61",
   "metadata": {},
   "source": [
    "## List of Unique Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c95451-5458-4d70-aedb-7ec9d87502cf",
   "metadata": {},
   "source": [
    "Next we are going to create a python list that contains all the unique words from all the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9c1599-f172-42a9-8ff5-71c5bf084875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                                SMS\n",
      "0   ham                       Yep, by the pretty sculpture\n",
      "1   ham      Yes, princess. Are you going to make me moan?\n",
      "2   ham                         Welp apparently he retired\n",
      "3   ham                                            Havent.\n",
      "4   ham  I forgot 2 ask ü all smth.. There's a card on ...\n",
      "\n",
      "  Label                                                SMS\n",
      "0   ham                        yep by the pretty sculpture\n",
      "1   ham        yes princess are you going to make me moan \n",
      "2   ham                         welp apparently he retired\n",
      "3   ham                                            havent \n",
      "4   ham  i forgot 2 ask ü all smth there s a card on da...\n"
     ]
    }
   ],
   "source": [
    "print(training_set.head(5))\n",
    "# \\W means all characters that are not a-z A-Z or 0-9\n",
    "training_set['SMS'] = training_set['SMS'].str.replace(r'\\W', \" \", regex=True)\n",
    "training_set['SMS'] = training_set['SMS'].str.lower()\n",
    "#delete extra spaces\n",
    "training_set['SMS'] = training_set['SMS'].str.replace(r'\\s+', ' ', regex=True)\n",
    "print('')\n",
    "print(training_set.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771c1f26-ab99-4877-8a3f-80a7e5ff1503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7783"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['SMS'] = training_set['SMS'].str.split()\n",
    "\n",
    "vocabulary = []\n",
    "for sms in training_set['SMS']:\n",
    "    for word in sms:\n",
    "        vocabulary.append(word)\n",
    "        \n",
    "vocabulary = list(set(vocabulary))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893929b1-6e8b-4637-9ee8-7a1a29444501",
   "metadata": {},
   "source": [
    "We now have a list of each unique word, overall there are 7783 unique words. Next we are going to use that vocabulary to make unique word columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0aa30-881e-4718-9f75-30db93c989e4",
   "metadata": {},
   "source": [
    "## The Final Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5691f27-b56e-4430-afd7-5b39c1000095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary with all unique words\n",
    "word_counts_per_sms = {unique_word: [0] * len(training_set['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(training_set['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a11bb21-c3db-4fc3-ad57-3153b4656486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4458, 7783)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5p</th>\n",
       "      <th>sport</th>\n",
       "      <th>actual</th>\n",
       "      <th>darlings</th>\n",
       "      <th>wallet</th>\n",
       "      <th>interview</th>\n",
       "      <th>jess</th>\n",
       "      <th>is</th>\n",
       "      <th>go</th>\n",
       "      <th>teasing</th>\n",
       "      <th>...</th>\n",
       "      <th>lick</th>\n",
       "      <th>08002988890</th>\n",
       "      <th>poo</th>\n",
       "      <th>area</th>\n",
       "      <th>21870000</th>\n",
       "      <th>among</th>\n",
       "      <th>unique</th>\n",
       "      <th>clocks</th>\n",
       "      <th>ur</th>\n",
       "      <th>person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   5p  sport  actual  darlings  wallet  interview  jess  is  go  teasing  ...  \\\n",
       "0   0      0       0         0       0          0     0   0   0        0  ...   \n",
       "1   0      0       0         0       0          0     0   0   0        0  ...   \n",
       "2   0      0       0         0       0          0     0   0   0        0  ...   \n",
       "3   0      0       0         0       0          0     0   0   0        0  ...   \n",
       "4   0      0       0         0       0          0     0   0   0        0  ...   \n",
       "\n",
       "   lick  08002988890  poo  area  21870000  among  unique  clocks  ur  person  \n",
       "0     0            0    0     0         0      0       0       0   0       0  \n",
       "1     0            0    0     0         0      0       0       0   0       0  \n",
       "2     0            0    0     0         0      0       0       0   0       0  \n",
       "3     0            0    0     0         0      0       0       0   0       0  \n",
       "4     0            0    0     0         0      0       0       0   0       0  \n",
       "\n",
       "[5 rows x 7783 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "print(word_counts.shape)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b1453-3f23-4267-91ad-ebcdfe62eb52",
   "metadata": {},
   "source": [
    "From the table above we can see that we have created a dataframe with 4458 rows and 7783 columns. Now we just need to add the `Label` and `SMS` columns and add all the word counts for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe4026eb-230e-42f9-a486-d27466962287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>5p</th>\n",
       "      <th>sport</th>\n",
       "      <th>actual</th>\n",
       "      <th>darlings</th>\n",
       "      <th>wallet</th>\n",
       "      <th>interview</th>\n",
       "      <th>jess</th>\n",
       "      <th>is</th>\n",
       "      <th>...</th>\n",
       "      <th>lick</th>\n",
       "      <th>08002988890</th>\n",
       "      <th>poo</th>\n",
       "      <th>area</th>\n",
       "      <th>21870000</th>\n",
       "      <th>among</th>\n",
       "      <th>unique</th>\n",
       "      <th>clocks</th>\n",
       "      <th>ur</th>\n",
       "      <th>person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  5p  sport  actual  \\\n",
       "0   ham                  [yep, by, the, pretty, sculpture]   0      0       0   \n",
       "1   ham  [yes, princess, are, you, going, to, make, me,...   0      0       0   \n",
       "2   ham                    [welp, apparently, he, retired]   0      0       0   \n",
       "3   ham                                           [havent]   0      0       0   \n",
       "4   ham  [i, forgot, 2, ask, ü, all, smth, there, s, a,...   0      0       0   \n",
       "\n",
       "   darlings  wallet  interview  jess  is  ...  lick  08002988890  poo  area  \\\n",
       "0         0       0          0     0   0  ...     0            0    0     0   \n",
       "1         0       0          0     0   0  ...     0            0    0     0   \n",
       "2         0       0          0     0   0  ...     0            0    0     0   \n",
       "3         0       0          0     0   0  ...     0            0    0     0   \n",
       "4         0       0          0     0   0  ...     0            0    0     0   \n",
       "\n",
       "   21870000  among  unique  clocks  ur  person  \n",
       "0         0      0       0       0   0       0  \n",
       "1         0      0       0       0   0       0  \n",
       "2         0      0       0       0   0       0  \n",
       "3         0      0       0       0   0       0  \n",
       "4         0      0       0       0   0       0  \n",
       "\n",
       "[5 rows x 7785 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_clean = pd.concat([training_set, word_counts], axis=1)\n",
    "training_set_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572cfcf4-bbc1-43cc-b123-62900b7ea744",
   "metadata": {},
   "source": [
    "## Calculating Constants First"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c29595-6144-41f6-a5ad-16b2e8a3f932",
   "metadata": {},
   "source": [
    "For Naive Bayes Algorithm we will need to know the probability values of P(spam given unique words) and P(non-spam given unique words). To avoid 0 values, we will use Laplace smoothing and add 1 to all the word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce068e0-dac3-4816-afbc-9c01b8882f37",
   "metadata": {},
   "source": [
    "Now we need to find the number of words in all the spam messages, number of words in all the non-spam messages and number of all unique words. We already found the number of unique words above, which was 7783. We can use the same code with filtering to get other number values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc2ea0b-9e85-4b0d-8f16-ff93d28d8828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam): 0.13458950201884254\n",
      "P(Ham): 0.8654104979811574\n",
      "Number of spam words: 15190\n",
      "Number of ham words: 57237\n",
      "Number of unique words: 7783\n"
     ]
    }
   ],
   "source": [
    "# Isolating spam and ham messages first\n",
    "spam_messages = training_set_clean[training_set_clean['Label'] == 'spam']\n",
    "ham_messages = training_set_clean[training_set_clean['Label'] == 'ham']\n",
    "\n",
    "# P(Spam) and P(Ham)\n",
    "p_spam = len(spam_messages) / len(training_set_clean)\n",
    "p_ham = len(ham_messages) / len(training_set_clean)\n",
    "\n",
    "# N_Spam\n",
    "n_words_per_spam_message = spam_messages['SMS'].apply(len)\n",
    "n_spam = n_words_per_spam_message.sum()\n",
    "\n",
    "# N_Ham\n",
    "n_words_per_ham_message = ham_messages['SMS'].apply(len)\n",
    "n_ham = n_words_per_ham_message.sum()\n",
    "\n",
    "# N_Vocabulary \n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1\n",
    "print(f\"P(Spam): {p_spam}\")\n",
    "print(f\"P(Ham): {p_ham}\")\n",
    "print(f\"Number of spam words: {n_spam}\")\n",
    "print(f\"Number of ham words: {n_ham}\")\n",
    "print(f\"Number of unique words: {n_vocabulary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e2a5a-7647-4853-b6e7-da7281c0e641",
   "metadata": {},
   "source": [
    "## Calculating Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9935cdfc-81c0-41fe-b4b0-177614a26e68",
   "metadata": {},
   "source": [
    "Next we need to calculate probabilities for each word, for example how often \"secret\" is in spam messages, and how often \"secret\" is in ham messages. This means that we have to calculate 15566 probabilities. Calculating these probabilities beforehand makes the algorithm more efficient when new message comes in, because it doesn't have to calculate them every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3ab0007-cf12-499d-aa88-f9f57b609980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3529360553693465e-05\n",
      "4.3529360553693465e-05\n",
      "0.0001305880816610804\n"
     ]
    }
   ],
   "source": [
    "# Initiate parameters\n",
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "# Calculate parameters\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam_messages[word].sum()   # spam_messages already defined in a cell above\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "    \n",
    "    n_word_given_ham = ham_messages[word].sum()   # ham_messages already defined in a cell above\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham\n",
    "\n",
    "#some examples\n",
    "print(parameters_spam['installing'])\n",
    "print(parameters_spam['edition'])\n",
    "print(parameters_spam['02073162414'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ebe97-35e2-495c-8d90-f34773835589",
   "metadata": {},
   "source": [
    "Now we have the dictionaries that have the probabilites for different words for both spam and ham."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad002f1-1f0d-47e8-8feb-00a9fd958471",
   "metadata": {},
   "source": [
    "## Classifying A New Message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba3552-4627-49d9-978b-dde03f72ecf1",
   "metadata": {},
   "source": [
    "Now that we have all our parameters calculated, we can start creating the spam filter. The spam filter can be understood as a function that:\n",
    "\n",
    "- Takes in as input a new message (w1, w2, ..., wn).\n",
    "- Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn).\n",
    "- Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n",
    "    - If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham.\n",
    "    - If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam.\n",
    "    - If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d11eeb63-e484-4f9a-9a73-6ad25ba4cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n",
      "ham\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def classify(message):\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "    \n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "            \n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "    \n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'Equal proabilities, have a human classify this!'\n",
    "#This should be spam        \n",
    "print(classify('WINNER!! This is the secret code to unlock the money: C3421.'))\n",
    "#This should be ham\n",
    "print(classify('Sounds good, Tom, then see u there'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bc1db-c492-45ea-8b76-ba64e2eca1d0",
   "metadata": {},
   "source": [
    "## Spam Filter's Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d68ab0-ac21-4377-958b-9afac1eea74d",
   "metadata": {},
   "source": [
    "Now that we have a working spam filter, we are going to test how well it works on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd4ed7ef-fcaf-4730-9f45-e55d63aa86fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS predicted\n",
       "0   ham          Later i guess. I needa do mcat study too.       ham\n",
       "1   ham             But i haf enuff space got like 4 mb...       ham\n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...      spam\n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...       ham\n",
       "4   ham  All done, all handed in. Don't know if mega sh...       ham"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set['predicted'] = testing_set['SMS'].apply(classify)\n",
    "testing_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d7a6d7-04ac-4531-b109-13c1b761a881",
   "metadata": {},
   "source": [
    "From the table above we can see the `Label` column that still contains labels selected by humans. The `predicted` column is our algorithm deciding whether or not it thinks the message was spam or ham. Let's check how accurate it is by dividing all the correct answers with the amount of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c92d448-9687-4596-a621-31e69c58bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answers: 1100\n",
      "Total answers: 1114\n",
      "Accuracy: 0.9874326750448833\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for index, row in testing_set.iterrows():\n",
    "    if row['Label'] == row['predicted']:\n",
    "        correct += 1\n",
    "        total += 1\n",
    "    else:\n",
    "        total += 1\n",
    "\n",
    "print(f\"Correct answers: {correct}\")\n",
    "print(f\"Total answers: {total}\")\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99a6a8-65fc-4d08-b83e-bbc7f269989e",
   "metadata": {},
   "source": [
    "We can see that the accuracy is very good, 98.7%. The goal was to make a filter that is at least 80% accurate, so this works a lot better than expected. Out of 1114 message our spam filter classified 1100 correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe25f9-4560-4aad-b691-1c3565a5800c",
   "metadata": {},
   "source": [
    "## Wrong Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9948aab9-3b70-4f6b-9fd3-c6dd9f7ada93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114: Not heard from U4 a while. Call me now am here all night with just my knickers on. Make me beg for it like U did last time 01223585236 XX Luv Nikiyu4.net\n",
      "135: More people are dogging in your area now. Call 09090204448 and join like minded guys. Why not arrange 1 yourself. There's 1 this evening. A£1.50 minAPN LS278BB\n",
      "152: Unlimited texts. Limited minutes.\n",
      "159: 26th OF JULY\n",
      "284: Nokia phone is lovly..\n",
      "293: A Boy loved a gal. He propsd bt she didnt mind. He gv lv lttrs, Bt her frnds threw thm. Again d boy decided 2 aproach d gal , dt time a truck was speeding towards d gal. Wn it was about 2 hit d girl,d boy ran like hell n saved her. She asked 'hw cn u run so fast?' D boy replied \"Boost is d secret of my energy\" n instantly d girl shouted \"our energy\" n Thy lived happily 2gthr drinking boost evrydy Moral of d story:- I hv free msgs:D;): gud ni8\n",
      "302: No calls..messages..missed calls\n",
      "319: We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us\n",
      "504: Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50\n",
      "546: Hi babe its Chloe, how r u? I was smashed on saturday night, it was great! How was your weekend? U been missing me? SP visionsms.com Text stop to stop 150p/text\n",
      "741: 0A$NETWORKS allow companies to bill for SMS, so they are responsible for their \"suppliers\", just as a shop has to give a guarantee on what they sell. B. G.\n",
      "876: RCT' THNQ Adrian for U text. Rgds Vatian\n",
      "885: 2/2 146tf150p\n",
      "953: Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r\n"
     ]
    }
   ],
   "source": [
    "for index, row in testing_set.iterrows():\n",
    "    if row['Label'] != row['predicted']:\n",
    "        print(f\"{index}: {row['SMS']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb93ba8-f458-42d5-9942-9f5eef8695ff",
   "metadata": {},
   "source": [
    "Above are all the wrong predictions. In my opinion, they are pretty hard to classify even as a human, so the algorithm worked very well. Most of the messages contain realistic-looking conversations, that are hard to identify as a spam, unless you know that they are promoting some sort of website or service, instead of just catching up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
